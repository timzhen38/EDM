{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-472cbcdb-4e9b-46f5-bc9e-f4b4e97c8bd3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5610,
    "execution_start": 1634168360884,
    "source_hash": "99e17f2b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/37159070/multiple-linear-regression-model-by-using-tensorflow\n",
    "# https://donaldpinckney.com/books/pytorch/book/ch2-linreg/2018-03-21-multi-variable.html\n",
    "# https://www.youtube.com/watch?v=Q4GNLhRtZNc\n",
    "# https://atmamani.github.io/projects/ml/coursera-gd-multivariate-linear-regression/\n",
    "# https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/05mlr/eq_matrix_notation/index.gif\n",
    "\n",
    "# Multivariable Logistic Regression for matricies.\n",
    "# target = flux1 + flux2 +... flux500 + b\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-a151b39c-c9e0-4cf9-b1a3-9643935f87c3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1634168366499,
    "source_hash": "cc523c23",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#normalize X values to help model converge\n",
    "def normalize(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    data = (data - mean) / std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-01b2c7d8-707d-4aa6-b424-ad8ecbbd1f58",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1634168366505,
    "source_hash": "93c1b31e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#retrieve and format data - into labels and examples from the dataset\n",
    "def features_and_labels(filename):\n",
    "    data = np.matrix(pd.read_csv(filename).values)\n",
    "\n",
    "    #we need to transpose data sets to be compatable with our weight vectors\n",
    "    data_y = data[:, 0].transpose()\n",
    "    data_y-=1 #binaryization of the categorical data\n",
    "\n",
    "    data_x = data[:, 1:].transpose()\n",
    "    data_x = normalize(data_x)\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00003-24aa0dac-d955-4941-8ed2-9ad14c317e71",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5067,
    "execution_start": 1634168366515,
    "source_hash": "dbbaced2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data_x, training_data_y = features_and_labels(\"../archive/exoTrain.csv\")\n",
    "\n",
    "#set hyperparameters & variables\n",
    "learning_rate = 0.003\n",
    "epochs = 500\n",
    "display_step = 5\n",
    "n_samples = training_data_x.shape[1]\n",
    "col_num = training_data_x.shape[0]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [col_num, n_samples])\n",
    "Y = tf.placeholder(tf.float32, [1, n_samples]) #resulting dimenstion of W*X matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00004-9f09321a-12c4-4c03-9d19-642a45a3f3b0",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 19,
    "execution_start": 1634168371590,
    "source_hash": "54f103e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want the weight vector to correspond one to one with every column\n",
    "W = tf.Variable(tf.zeros([1,col_num], dtype=np.float32), name=\"weight\")\n",
    "b = tf.Variable(tf.zeros([1, ], dtype=np.float32), name=\"bias\")\n",
    "\n",
    "#matrix multiplication requires outer dimension of W to be equal to be equal to the inner dimension of X: \n",
    "# (1,col_num) & (col_num, num_samples) - this is why we transpose X\n",
    "pred = tf.matmul(W, X) + b # yâ€²(x,A,b)=Ax+b linear matrix equation\n",
    "\n",
    "error = tf.reduce_sum((pred-Y)**2) / (n_samples * 2) #MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-5636fef9-095b-4767-984f-a967c95b5804",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 81157,
    "execution_start": 1634168371618,
    "source_hash": "e943c521",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0, loss = 0.00363672, W = [[1.20844725e-05 1.08624172e-05 6.64995059e-06 ... 2.96250755e-06\n",
      "  5.31442720e-06 6.92352614e-06]], b = [2.1820326e-05]\n",
      "t = 5, loss = 0.00356816, W = [[7.8847261e-05 7.0592912e-05 4.6259072e-05 ... 2.3039685e-05\n",
      "  3.6960933e-05 4.6242436e-05]], b = [0.00013327]\n",
      "t = 10, loss = 0.00352213, W = [[1.4821738e-04 1.3214620e-04 8.8468623e-05 ... 4.5107339e-05\n",
      "  7.0611262e-05 8.7744906e-05]], b = [0.00024347]\n",
      "t = 15, loss = 0.00348207, W = [[2.1559105e-04 1.9126640e-04 1.2876095e-04 ... 6.7038280e-05\n",
      "  1.0422215e-04 1.2963693e-04]], b = [0.00035196]\n",
      "t = 20, loss = 0.00344504, W = [[2.8007483e-04 2.4720989e-04 1.6630910e-04 ... 8.8442190e-05\n",
      "  1.3736074e-04 1.7138754e-04]], b = [0.00045895]\n",
      "t = 25, loss = 0.00341012, W = [[0.00034164 0.00030004 0.00020114 ... 0.00010924 0.00016991 0.00021277]], b = [0.00056453]\n",
      "t = 30, loss = 0.00337687, W = [[0.00040044 0.00034996 0.00023348 ... 0.00012944 0.00020185 0.00025368]], b = [0.00066875]\n",
      "t = 35, loss = 0.00334507, W = [[0.00045669 0.00039724 0.00026356 ... 0.00014906 0.0002332  0.00029409]], b = [0.00077163]\n",
      "t = 40, loss = 0.00331454, W = [[0.00051059 0.00044209 0.00029161 ... 0.00016815 0.00026398 0.00033399]], b = [0.00087319]\n",
      "t = 45, loss = 0.00328518, W = [[0.00056232 0.00048472 0.00031783 ... 0.00018674 0.00029421 0.00037338]], b = [0.00097345]\n",
      "t = 50, loss = 0.00325691, W = [[0.00061204 0.00052532 0.00034239 ... 0.00020484 0.00032393 0.00041226]], b = [0.00107242]\n",
      "t = 55, loss = 0.00322963, W = [[0.00065991 0.00056404 0.00036544 ... 0.00022248 0.00035314 0.00045065]], b = [0.00117011]\n",
      "t = 60, loss = 0.00320329, W = [[0.00070605 0.00060101 0.00038711 ... 0.00023968 0.00038186 0.00048856]], b = [0.00126655]\n",
      "t = 65, loss = 0.00317784, W = [[0.00075058 0.00063636 0.0004075  ... 0.00025645 0.00041011 0.00052599]], b = [0.00136174]\n",
      "t = 70, loss = 0.00315322, W = [[0.00079358 0.00067019 0.00042672 ... 0.0002728  0.0004379  0.00056296]], b = [0.0014557]\n",
      "t = 75, loss = 0.00312939, W = [[0.00083516 0.0007026  0.00044483 ... 0.00028874 0.00046523 0.00059947]], b = [0.00154846]\n",
      "t = 80, loss = 0.0031063, W = [[0.00087538 0.00073368 0.00046192 ... 0.00030429 0.00049213 0.00063553]], b = [0.00164003]\n",
      "t = 85, loss = 0.00308392, W = [[0.00091433 0.00076349 0.00047805 ... 0.00031945 0.00051859 0.00067115]], b = [0.00173044]\n",
      "t = 90, loss = 0.00306222, W = [[0.00095206 0.0007921  0.00049328 ... 0.00033422 0.00054463 0.00070635]], b = [0.00181969]\n",
      "t = 95, loss = 0.00304115, W = [[0.00098864 0.00081958 0.00050766 ... 0.00034863 0.00057027 0.00074113]], b = [0.00190781]\n",
      "t = 100, loss = 0.0030207, W = [[0.00102412 0.00084599 0.00052124 ... 0.00036268 0.0005955  0.0007755 ]], b = [0.00199481]\n",
      "t = 105, loss = 0.00300083, W = [[0.00105856 0.00087138 0.00053407 ... 0.00037638 0.00062033 0.00080947]], b = [0.00208072]\n",
      "t = 110, loss = 0.00298151, W = [[0.001092   0.0008958  0.00054618 ... 0.00038973 0.00064479 0.00084306]], b = [0.00216556]\n",
      "t = 115, loss = 0.00296272, W = [[0.00112448 0.00091929 0.00055762 ... 0.00040274 0.00066887 0.00087626]], b = [0.00224934]\n",
      "t = 120, loss = 0.00294445, W = [[0.00115605 0.0009419  0.00056841 ... 0.00041543 0.00069258 0.00090909]], b = [0.00233207]\n",
      "t = 125, loss = 0.00292666, W = [[0.00118675 0.00096366 0.0005786  ... 0.0004278  0.00071594 0.00094155]], b = [0.00241379]\n",
      "t = 130, loss = 0.00290933, W = [[0.0012166  0.00098462 0.00058821 ... 0.00043985 0.00073895 0.00097367]], b = [0.00249449]\n",
      "t = 135, loss = 0.00289245, W = [[0.00124566 0.00100481 0.00059728 ... 0.0004516  0.00076161 0.00100543]], b = [0.00257421]\n",
      "t = 140, loss = 0.002876, W = [[0.00127394 0.00102427 0.00060583 ... 0.00046306 0.00078395 0.00103686]], b = [0.00265296]\n",
      "t = 145, loss = 0.00285996, W = [[0.00130149 0.00104302 0.00061389 ... 0.00047422 0.00080596 0.00106796]], b = [0.00273075]\n",
      "t = 150, loss = 0.00284432, W = [[0.00132832 0.00106109 0.00062147 ... 0.0004851  0.00082765 0.00109873]], b = [0.0028076]\n",
      "t = 155, loss = 0.00282906, W = [[0.00135446 0.00107851 0.00062861 ... 0.0004957  0.00084904 0.00112919]], b = [0.00288352]\n",
      "t = 160, loss = 0.00281416, W = [[0.00137995 0.00109532 0.00063533 ... 0.00050603 0.00087012 0.00115934]], b = [0.00295854]\n",
      "t = 165, loss = 0.00279961, W = [[0.00140481 0.00111152 0.00064165 ... 0.0005161  0.00089091 0.00118919]], b = [0.00303266]\n",
      "t = 170, loss = 0.00278541, W = [[0.00142906 0.00112716 0.00064758 ... 0.00052592 0.00091141 0.00121874]], b = [0.00310591]\n",
      "t = 175, loss = 0.00277152, W = [[0.00145271 0.00114224 0.00065314 ... 0.00053548 0.00093163 0.00124801]], b = [0.00317828]\n",
      "t = 180, loss = 0.00275796, W = [[0.0014758  0.0011568  0.00065836 ... 0.00054479 0.00095157 0.00127699]], b = [0.00324981]\n",
      "t = 185, loss = 0.00274469, W = [[0.00149835 0.00117084 0.00066325 ... 0.00055387 0.00097124 0.0013057 ]], b = [0.0033205]\n",
      "t = 190, loss = 0.00273172, W = [[0.00152036 0.0011844  0.00066782 ... 0.00056272 0.00099065 0.00133413]], b = [0.00339037]\n",
      "t = 195, loss = 0.00271903, W = [[0.00154187 0.00119749 0.00067209 ... 0.00057133 0.0010098  0.0013623 ]], b = [0.00345943]\n",
      "t = 200, loss = 0.00270661, W = [[0.00156288 0.00121012 0.00067607 ... 0.00057973 0.0010287  0.00139021]], b = [0.0035277]\n",
      "t = 205, loss = 0.00269446, W = [[0.00158342 0.00122232 0.00067978 ... 0.0005879  0.00104736 0.00141786]], b = [0.00359518]\n",
      "t = 210, loss = 0.00268256, W = [[0.00160349 0.00123409 0.00068323 ... 0.00059587 0.00106577 0.00144527]], b = [0.00366189]\n",
      "t = 215, loss = 0.0026709, W = [[0.00162313 0.00124547 0.00068643 ... 0.00060362 0.00108395 0.00147243]], b = [0.00372784]\n",
      "t = 220, loss = 0.00265949, W = [[0.00164232 0.00125644 0.00068939 ... 0.00061118 0.0011019  0.00149935]], b = [0.00379304]\n",
      "t = 225, loss = 0.0026483, W = [[0.00166111 0.00126705 0.00069212 ... 0.00061853 0.00111962 0.00152603]], b = [0.00385751]\n",
      "t = 230, loss = 0.00263734, W = [[0.00167948 0.00127728 0.00069464 ... 0.0006257  0.00113712 0.00155249]], b = [0.00392126]\n",
      "t = 235, loss = 0.00262659, W = [[0.00169747 0.00128717 0.00069696 ... 0.00063267 0.00115441 0.00157871]], b = [0.0039843]\n",
      "t = 240, loss = 0.00261605, W = [[0.00171507 0.00129671 0.00069907 ... 0.00063946 0.00117149 0.00160472]], b = [0.00404663]\n",
      "t = 245, loss = 0.00260571, W = [[0.00173231 0.00130593 0.000701   ... 0.00064607 0.00118835 0.0016305 ]], b = [0.00410828]\n",
      "t = 250, loss = 0.00259557, W = [[0.00174919 0.00131483 0.00070275 ... 0.0006525  0.00120502 0.00165608]], b = [0.00416925]\n",
      "t = 255, loss = 0.00258562, W = [[0.00176572 0.00132343 0.00070433 ... 0.00065876 0.00122148 0.00168144]], b = [0.00422956]\n",
      "t = 260, loss = 0.00257585, W = [[0.00178192 0.00133173 0.00070575 ... 0.00066485 0.00123775 0.00170659]], b = [0.0042892]\n",
      "t = 265, loss = 0.00256626, W = [[0.00179779 0.00133974 0.00070701 ... 0.00067077 0.00125383 0.00173154]], b = [0.0043482]\n",
      "t = 270, loss = 0.00255685, W = [[0.00181334 0.00134747 0.00070812 ... 0.00067653 0.00126973 0.00175629]], b = [0.00440656]\n",
      "t = 275, loss = 0.0025476, W = [[0.00182858 0.00135494 0.00070909 ... 0.00068214 0.00128543 0.00178085]], b = [0.0044643]\n",
      "t = 280, loss = 0.00253852, W = [[0.00184352 0.00136215 0.00070992 ... 0.00068759 0.00130096 0.00180521]], b = [0.00452142]\n",
      "t = 285, loss = 0.0025296, W = [[0.00185817 0.0013691  0.00071063 ... 0.00069289 0.00131632 0.00182938]], b = [0.00457793]\n",
      "t = 290, loss = 0.00252083, W = [[0.00187253 0.00137581 0.00071121 ... 0.00069804 0.0013315  0.00185336]], b = [0.00463384]\n",
      "t = 295, loss = 0.00251222, W = [[0.00188662 0.00138229 0.00071167 ... 0.00070305 0.00134651 0.00187716]], b = [0.00468917]\n",
      "t = 300, loss = 0.00250374, W = [[0.00190044 0.00138854 0.00071202 ... 0.00070791 0.00136135 0.00190078]], b = [0.00474391]\n",
      "t = 305, loss = 0.00249542, W = [[0.001914   0.00139456 0.00071227 ... 0.00071264 0.00137603 0.00192422]], b = [0.00479808]\n",
      "t = 310, loss = 0.00248722, W = [[0.0019273  0.00140037 0.00071241 ... 0.00071723 0.00139055 0.00194749]], b = [0.00485169]\n",
      "t = 315, loss = 0.00247917, W = [[0.00194035 0.00140598 0.00071246 ... 0.00072169 0.00140492 0.00197058]], b = [0.00490475]\n",
      "t = 320, loss = 0.00247124, W = [[0.00195316 0.00141138 0.00071241 ... 0.00072602 0.00141913 0.0019935 ]], b = [0.00495725]\n",
      "t = 325, loss = 0.00246344, W = [[0.00196574 0.00141659 0.00071227 ... 0.00073022 0.00143318 0.00201626]], b = [0.00500923]\n",
      "t = 330, loss = 0.00245577, W = [[0.00197808 0.0014216  0.00071205 ... 0.00073429 0.00144709 0.00203885]], b = [0.00506067]\n",
      "t = 335, loss = 0.00244822, W = [[0.0019902  0.00142643 0.00071175 ... 0.00073825 0.00146086 0.00206128]], b = [0.00511159]\n",
      "t = 340, loss = 0.00244078, W = [[0.0020021  0.00143109 0.00071137 ... 0.00074208 0.00147448 0.00208355]], b = [0.00516199]\n",
      "t = 345, loss = 0.00243346, W = [[0.00201378 0.00143556 0.00071092 ... 0.0007458  0.00148795 0.00210566]], b = [0.00521189]\n",
      "t = 350, loss = 0.00242625, W = [[0.00202526 0.00143987 0.0007104  ... 0.0007494  0.00150129 0.00212762]], b = [0.00526129]\n",
      "t = 355, loss = 0.00241916, W = [[0.00203654 0.00144402 0.00070982 ... 0.00075289 0.0015145  0.00214943]], b = [0.0053102]\n",
      "t = 360, loss = 0.00241216, W = [[0.00204761 0.00144801 0.00070917 ... 0.00075627 0.00152757 0.00217108]], b = [0.00535862]\n",
      "t = 365, loss = 0.00240527, W = [[0.00205849 0.00145184 0.00070846 ... 0.00075955 0.00154051 0.00219258]], b = [0.00540657]\n",
      "t = 370, loss = 0.00239849, W = [[0.00206918 0.00145553 0.00070769 ... 0.00076271 0.00155332 0.00221394]], b = [0.00545404]\n",
      "t = 375, loss = 0.0023918, W = [[0.00207969 0.00145906 0.00070687 ... 0.00076578 0.001566   0.00223516]], b = [0.00550105]\n",
      "t = 380, loss = 0.00238521, W = [[0.00209001 0.00146246 0.000706   ... 0.00076874 0.00157856 0.00225623]], b = [0.0055476]\n",
      "t = 385, loss = 0.00237871, W = [[0.00210016 0.00146572 0.00070508 ... 0.0007716  0.001591   0.00227716]], b = [0.00559371]\n",
      "t = 390, loss = 0.0023723, W = [[0.00211013 0.00146885 0.00070412 ... 0.00077437 0.00160331 0.00229795]], b = [0.00563937]\n",
      "t = 395, loss = 0.00236599, W = [[0.00211994 0.00147184 0.00070311 ... 0.00077704 0.00161551 0.00231861]], b = [0.00568459]\n",
      "t = 400, loss = 0.00235976, W = [[0.00212957 0.00147471 0.00070206 ... 0.00077961 0.00162759 0.00233913]], b = [0.00572938]\n",
      "t = 405, loss = 0.00235362, W = [[0.00213905 0.00147745 0.00070097 ... 0.00078209 0.00163956 0.00235952]], b = [0.00577374]\n",
      "t = 410, loss = 0.00234756, W = [[0.00214837 0.00148008 0.00069984 ... 0.00078449 0.00165141 0.00237978]], b = [0.00581769]\n",
      "t = 415, loss = 0.00234158, W = [[0.00215753 0.00148259 0.00069868 ... 0.00078679 0.00166315 0.0023999 ]], b = [0.00586122]\n",
      "t = 420, loss = 0.00233569, W = [[0.00216654 0.00148499 0.00069749 ... 0.00078901 0.00167478 0.0024199 ]], b = [0.00590434]\n",
      "t = 425, loss = 0.00232987, W = [[0.0021754  0.00148727 0.00069626 ... 0.00079115 0.00168631 0.00243978]], b = [0.00594706]\n",
      "t = 430, loss = 0.00232413, W = [[0.00218412 0.00148945 0.000695   ... 0.0007932  0.00169773 0.00245952]], b = [0.00598938]\n",
      "t = 435, loss = 0.00231846, W = [[0.0021927  0.00149153 0.00069372 ... 0.00079517 0.00170904 0.00247915]], b = [0.00603132]\n",
      "t = 440, loss = 0.00231286, W = [[0.00220113 0.0014935  0.00069241 ... 0.00079706 0.00172026 0.00249865]], b = [0.00607287]\n",
      "t = 445, loss = 0.00230734, W = [[0.00220943 0.00149538 0.00069108 ... 0.00079887 0.00173137 0.00251804]], b = [0.00611403]\n",
      "t = 450, loss = 0.00230189, W = [[0.00221759 0.00149716 0.00068972 ... 0.0008006  0.00174238 0.0025373 ]], b = [0.00615483]\n",
      "t = 455, loss = 0.00229651, W = [[0.00222562 0.00149885 0.00068835 ... 0.00080226 0.0017533  0.00255645]], b = [0.00619525]\n",
      "t = 460, loss = 0.00229119, W = [[0.00223353 0.00150044 0.00068695 ... 0.00080385 0.00176412 0.00257548]], b = [0.00623531]\n",
      "t = 465, loss = 0.00228594, W = [[0.0022413  0.00150195 0.00068553 ... 0.00080536 0.00177484 0.0025944 ]], b = [0.006275]\n",
      "t = 470, loss = 0.00228076, W = [[0.00224896 0.00150337 0.0006841  ... 0.0008068  0.00178547 0.0026132 ]], b = [0.00631435]\n",
      "t = 475, loss = 0.00227564, W = [[0.00225649 0.00150471 0.00068265 ... 0.00080817 0.00179601 0.0026319 ]], b = [0.00635334]\n",
      "t = 480, loss = 0.00227058, W = [[0.0022639  0.00150596 0.00068119 ... 0.00080947 0.00180646 0.00265048]], b = [0.00639199]\n",
      "t = 485, loss = 0.00226558, W = [[0.0022712  0.00150714 0.00067971 ... 0.0008107  0.00181682 0.00266895]], b = [0.00643029]\n",
      "t = 490, loss = 0.00226064, W = [[0.00227838 0.00150823 0.00067822 ... 0.00081187 0.00182709 0.00268732]], b = [0.00646826]\n",
      "t = 495, loss = 0.00225576, W = [[0.00228545 0.00150926 0.00067672 ... 0.00081298 0.00183727 0.00270558]], b = [0.0065059]\n",
      "Optimization Finished!\n",
      "Training error= 0.0022509356 W= [[0.00229102 0.00151002 0.00067551 ... 0.00081381 0.00184536 0.00272011]] b= [0.00653577] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for t in range(epochs):\n",
    "    \n",
    "    _, current_loss, current_W, current_b = session.run([optimizer, error, W, b], feed_dict={\n",
    "        X: training_data_x,\n",
    "        Y: training_data_y\n",
    "    })\n",
    "\n",
    "    if t % display_step == 0:\n",
    "        print(\"t = %g, loss = %g, W = %s, b = %s\" % (t, current_loss, str(current_W), str(current_b)))\n",
    "    \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "training_error = session.run(error, feed_dict={X: training_data_x, Y: training_data_y})\n",
    "print(\"Training error=\", training_error, \"W=\", session.run(W), \"b=\", session.run(b), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-f2dc2ae5-f310-4a14-8d71-88384e4f6257",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1634168452778,
    "source_hash": "27c812bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Logistic Layer using a sigmoid function\n",
    "def logistic_layer(y):\n",
    "    y = np.array(y)\n",
    "    y = 1 / (1 + exp(-y)) # sigmoid function\n",
    "    y = y.ravel()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-b837a0bd-a370-4dec-80f9-4f68a9cd9f78",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1634168452789,
    "source_hash": "b7072dad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate an accuracy metric\n",
    "def accuracy(predicted_y, true_y):\n",
    "    true_y = np.array(true_y).ravel()\n",
    "    counter = 0\n",
    "    for i in range(len(true_y)):\n",
    "        p_y = predicted_y[i]\n",
    "        t_y = true_y[i]\n",
    "        if (p_y>.5 and t_y == 1) or (p_y < .5 and t_y == 0):\n",
    "            counter+=1\n",
    "    counter = (counter/ len(true_y)) * 100\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-614e8ce1-d4bc-40d6-b8f7-3e04cbff6586",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1079,
    "execution_start": 1634168452795,
    "source_hash": "a84f9185",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy percentage:  52.63157894736842 %\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = features_and_labels(\"../archive/exoTest.csv\")\n",
    "\n",
    "predicted_y = session.run(W) * test_x + session.run(b)\n",
    "predicted_y = logistic_layer(predicted_y)\n",
    "\n",
    "print(\"Accuracy percentage: \", accuracy(predicted_y, test_y), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=1c58af11-5d4f-40f3-a808-e868f2c28485' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9816afbc-27fe-4cd8-82a2-1b252605b243",
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
